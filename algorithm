
# This code is part of a project developed to analyze data 
# and generate actionable insights. Portions of the algorithm are withheld to 
# protect proprietary methods and intellectual property.
# 
# Publicly shared components:
# - Data fetching 
# - Basic data preprocessing
# 
# Sensitive components, including the core logic,
# are not included in this version to maintain confidentiality.
# 
# For inquiries or collaboration opportunities, please contact:
# - LinkedIn: www.linkedin.com/in/oscar-gracia-2983952a2
# - Email: contact@oscargracia.site
# - Website: oscargracia.site
#
#
#This code is made available for educational and informational purposes only. You are not authorized 
#to use, reproduce, distribute, or modify this code for commercial or competitive purposes. Any 
#unauthorized use is prohibited.
#
#The repository does not include all components of the original project. Confidential portions have 
#been omitted to protect intellectual property.
#



import warnings
import ccxt
import pandas as pd
import numpy as np
import statsmodels.api as sm
import mplfinance as mpf
from itertools import combinations
import itertools
import math
from scipy.stats import linregress
import time
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from pandas import Timestamp
from datetime import datetime, timedelta, timezone
from math import ceil
from statsmodels.tools.sm_exceptions import ConvergenceWarning
import time
import threading
import requests
from requests.auth import HTTPBasicAuth
import re
import statsmodels.formula.api as smf
import asyncio
from telegram import Bot
import fcntl
import os


#Initialize Binance
binance = ccxt.binanceusdm()  # Binance US Dollar Margin Futures


def initial_start_end_times():
    
    ######################
    def get_current_time_now_rounded():
        # Get the current UTC time
        now_utc = datetime.utcnow()

        # Round down to the nearest minute by subtracting the current seconds (and microseconds)
        rounded_utc = now_utc - timedelta(seconds=now_utc.second, microseconds=now_utc.microsecond)

        #print(f"Current UTC time, rounded down to the nearest minute: {rounded_utc}")
        return rounded_utc

    ######################
    current_timenow_firstrun = get_current_time_now_rounded()

    # Convert to the specified format
    end_date_time_formatted_string = current_timenow_firstrun.strftime('%Y-%m-%d %H:%M:%S')
    
    #end_date_time_formatted_string
    
    #YYYY-MM-DD HH:MM:SS
    #4h start end time for analysis
    #start_date_time = '2023-10-23 00:00:00'
    end_date_time = end_date_time_formatted_string
    #end_date_time = '2024-02-24 01:22:00'
    #'2024-01-23 14:45:00'

    ######################
    def convert_to_iso8601_utc(datetime_str):
        """
        Convert a datetime string from 'YYYY-MM-DD HH:MM:SS' format to ISO 8601 format with 'Z' (UTC).

        Parameters:
        - datetime_str (str): The datetime string in 'YYYY-MM-DD HH:MM:SS' format.

        Returns:
        - str: The datetime in ISO 8601 format with 'Z' indicating UTC timezone.
        """
        # Parse the input string to a datetime object
        dt_object = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')

        # Format the datetime object to ISO 8601 format and append 'Z'
        formatted_datetime = dt_object.strftime('%Y-%m-%dT%H:%M:%S') + 'Z'

        return formatted_datetime

    # Example usage:
    #end_date_time = '2024-01-23 14:45:00'
    iso8601_end_datetime = convert_to_iso8601_utc(end_date_time)
    #print(iso8601_end_datetime)


    ######################
    def convert_iso8601_and_subtract_hours(iso8601_datetime, hours):
        """
        Convert an ISO 8601 datetime string to the same format but hours into the past.

        Parameters:
        - iso8601_datetime (str): The ISO 8601 datetime string.
        - hours (int): Number of hours to subtract.

        Returns:
        - str: The adjusted datetime in ISO 8601 format with 'Z'.
        """
        # Parse the datetime, handling the 'Z' for UTC
        if iso8601_datetime.endswith('Z'):
            dt_object = datetime.fromisoformat(iso8601_datetime.rstrip('Z')).replace(tzinfo=timezone.utc)
        else:
            dt_object = datetime.fromisoformat(iso8601_datetime).replace(tzinfo=timezone.utc)

        # Subtract the hours using timedelta
        adjusted_dt_object = dt_object - timedelta(hours=hours)

        # Convert back to ISO 8601 format, ensuring it ends with 'Z'
        adjusted_iso8601_datetime = adjusted_dt_object.isoformat(timespec='seconds').replace('+00:00', '') + 'Z'

        return adjusted_iso8601_datetime


    # Example usage:
    #iso8601_end_datetime = '2024-01-23T14:45:00Z'
    start_iso8601_datetime = convert_iso8601_and_subtract_hours(iso8601_end_datetime, 2000)
    #print(start_iso8601_datetime)

    ######################
    #Initialize Binance
    #binance = ccxt.binanceusdm()  # Binance US Dollar Margin Futures
    #total start and end time
    start_time = binance.parse8601(start_iso8601_datetime)
    end_time = binance.parse8601(iso8601_end_datetime)

    return start_time, end_time, end_date_time


# Function to check if data is complete
def is_data_complete(ohlcv, expected_interval):
    """
    Check if the OHLCV data is complete for the given expected interval.

    Parameters:
    - ohlcv: List of OHLCV data.
    - expected_interval: The expected interval between each data point in milliseconds.

    Returns:
    - bool: True if data is complete, False otherwise.
    """
    for i in range(len(ohlcv) - 1):
        if (ohlcv[i + 1][0] - ohlcv[i][0]) != expected_interval:
            return False
    return True


# Function to fetch data
def fetch_data(exchange, symbol, timeframe, start_time, end_time, interval_milliseconds):
    """
    Fetches OHLCV data for the specified symbol and timeframe from the exchange.

    Parameters:
    - exchange: Exchange object to fetch data from.
    - symbol: Symbol to fetch data for.
    - timeframe: Timeframe for the data.
    - start_time: Start time in milliseconds since epoch.
    - end_time: End time in milliseconds since epoch.
    - interval_milliseconds: Interval between each data point in milliseconds.

    Returns:
    - df: DataFrame containing the fetched data.
    """
    compute_start_time = time.time()  # Start time recording
    data = []
    limit = 1000
    while start_time < end_time:
        try:
            ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since=start_time, limit=limit)
            if not ohlcv:
                break
            if not is_data_complete(ohlcv, interval_milliseconds):
                print("Warning: Incomplete or irregular data detected")
                break
            last_time = ohlcv[-1][0]
            if last_time > end_time:
                ohlcv = [candle for candle in ohlcv if candle[0] <= end_time]
                data += ohlcv
                break
            data += ohlcv
            start_time = last_time + interval_milliseconds  # Adjust for next fetch
        except Exception as e:
            print(f"An error occurred: {e}")
            break

    df = pd.DataFrame(data, columns=['date', 'open', 'high', 'low', 'close', 'volume'])
    df['date'] = pd.to_datetime(df['date'], unit='ms')
    df.set_index('date', inplace=True)
    compute_end_time = time.time()  # End time recording
    compute_time_taken = compute_end_time - compute_start_time  # Calculate time taken
    #print(f"Time taken to compute the function fetch data: {compute_time_taken:.4f} seconds")
    return df


# Fetch data
#df = fetch_data(binance, 'BTC/USDT', '4h', start_time, end_time, 14400000)

# Directly add the 'time_count' column to the original DataFrame
#df['time_count'] = (((df.index - df.index.min()).total_seconds()) / (240*60)).astype(int)

# Directly add the 'time_count' column to the filtered DataFrame
#filtered_df['time_count'] = (((filtered_df.index - filtered_df.index.min()).total_seconds()) / (240*60)).astype(int)

# Display the first few rows of the DataFrame
#df


def count_intervals(df, interval_minutes):
    """
    Adds a column to a DataFrame counting specified minute intervals.

    Parameters:
    - df: pd.DataFrame, the original DataFrame with a datetime index.
    - interval_minutes: int, the length of the interval in minutes.

    Returns:
    - pd.DataFrame, the DataFrame with an additional 'time_count' column.
    """

    # Ensure the DataFrame's index is in datetime format
    if not pd.api.types.is_datetime64_any_dtype(df.index):
        df.index = pd.to_datetime(df.index)

    # Calculate the number of specified minute intervals since the start of the DataFrame
    df['time_count'] = (((df.index - df.index.min()).total_seconds()) / (interval_minutes * 60)).astype(int)

    return df

# Assuming 'df' is your DataFrame with a datetime index

# For counting 30-minute intervals
#df_with_30m_intervals = count_intervals(df, 30)

# For counting 5-minute intervals
#df_with_5m_intervals = count_intervals(df, 5)

# For counting 1-minute intervals
#df_with_1m_intervals = count_intervals(df, 1)

# Display the resulting DataFrame for one of the intervals
#print(df_with_30m_intervals.head())  # Change as needed to view others

def convert_4h_milliseconds_unix_to_iso_8601(timestamp_ms):
    # Given timestamp in milliseconds
    #timestamp_ms = 1718640060000
    
    # Convert milliseconds to seconds
    timestamp_sec = timestamp_ms / 1000
    
    # Convert to datetime object in UTC
    datetime_obj = datetime.datetime.utcfromtimestamp(timestamp_sec)
    
    # Format the datetime object in ISO 8601 format
    iso_format = datetime_obj.strftime('%Y-%m-%dT%H:%M:%SZ')
    
    #print(iso_format)

    return datetime_obj

def initial_fetch_4h():
    
    # Fetch data
    #Initialize Binance
    #binance = ccxt.binanceusdm()  # Binance US Dollar Margin Futures
    start_time, end_time, end_date_time = initial_start_end_times()
    #mandar al telegram el start time y end time
    result_4h_start_time_telegram_send_variable_with_prefix = telegram_send_variable_with_prefix(start_time, '4h', '4h_start_time:')
    result_4h_end_time_telegram_send_variable_with_prefix = telegram_send_variable_with_prefix(end_time, '4h', '4h_end_time:')
    result_4h_end_date_time_telegram_send_variable_with_prefix = telegram_send_variable_with_prefix(end_date_time, '4h', '4h_end_date_time:')
    four_hours_in_milliseconds = 14400000
    df = fetch_data(binance, 'BTC/USDT', '4h', start_time, end_time, four_hours_in_milliseconds)
    filtered_df = count_intervals(df, (4 * 60))
    
    return filtered_df, end_date_time


def format_datetime_intervals(filtered_segments, filtered_df, end_date_time):
    """
    Formats the start and end datetime intervals based on the last value in filtered_segments and a given end datetime.

    Args:
        filtered_segments (list of tuples): A list containing tuples, where each tuple represents a segment.
        filtered_df (pd.DataFrame): A DataFrame containing a 'time_count' column to match with the last tuple's first value.
        end_date_time (str): The end datetime string in '%Y-%m-%d %H:%M:%S' format.

    Returns:
        tuple of str: A tuple containing formatted start and end datetime strings in '%Y-%m-%dT%H:%M:%SZ' format.
    """
    # Access the first value of the last tuple in filtered_segments
    first_value_of_last_tuple = filtered_segments[-1][0]

    # Find the index where 'time_count' matches 'first_value_of_last_tuple'
    matching_row = filtered_df[filtered_df['time_count'] == first_value_of_last_tuple]

    if not matching_row.empty:
        date_time_index = matching_row.index[0]  # Extract the first (or only) matching index.
    else:
        raise ValueError("No matching row found in the DataFrame.")

    # Ensure the 'date_time_index' is a datetime object before formatting
    if not isinstance(date_time_index, pd.Timestamp):
        date_time_index = pd.to_datetime(date_time_index)

    # Format start datetime
    formatted_start_date_time = date_time_index.strftime('%Y-%m-%dT%H:%M:%SZ')

    # Convert end_date_time to datetime object and format
    dt_object_2 = datetime.strptime(end_date_time, '%Y-%m-%d %H:%M:%S')
    formatted_end_date_time = dt_object_2.strftime('%Y-%m-%dT%H:%M:%SZ')

    return formatted_start_date_time, formatted_end_date_time, end_date_time


def process_dataframe(input_df):
    """
    Processes the input DataFrame by dropping specific columns and retaining others.
    
    Parameters:
    - input_df: pandas DataFrame to be processed.
    
    Returns:
    - A deep copy of the processed DataFrame with specified columns removed.
    """
    cols_to_drop = ['Start_X', 'Start_Y', 'End_X']  # 'Line' is not dropped here
    cols_to_drop = [col for col in cols_to_drop if col in input_df.columns]
    processed_df = input_df.drop(columns=cols_to_drop).copy(deep=True)
    
    return processed_df


def process_dataframe_2(df):
    """
    Process the dataframe by applying multiple cleaning and renaming operations:
    1. Drops columns based on specified suffixes.
    2. Renames columns by removing certain prefixes and suffixes.
    3. Extracts and remaps line IDs in column names.
    4. Converts all column names to lowercase.

    Parameters:
    df (pd.DataFrame): The original dataframe to process.

    Returns:
    pd.DataFrame: The processed dataframe.
    """

    # Drop columns that end with specified suffixes
    suffixes_to_drop = ['End_Y', '95_Line_Length_X', '95_Width_Start', '95_Width_End']
    cols_to_drop = [col for col in df.columns if any(col.endswith(suffix) for suffix in suffixes_to_drop)]
    df = df.drop(columns=cols_to_drop)

    # Rename columns by removing '05_' from those that end with specified suffixes
    suffixes_to_rename = ['05_Line_Length_X', '05_Width_Start', '05_Width_End']
    cols_to_rename = {col: col.replace('05_', '') for col in df.columns if any(col.endswith(suffix) for suffix in suffixes_to_rename)}
    df = df.rename(columns=cols_to_rename)

    # Remove 'QR_' from the start of each column name
    cols_remove_qr = {col: col.replace('QR_', '') for col in df.columns if col.startswith('QR_')}
    df = df.rename(columns=cols_remove_qr)

    # Extract leading numbers from column names as line IDs and remap
    line_ids = {col: int(re.match(r'\d+', col).group()) for col in df.columns if re.match(r'\d+', col)}
    unique_sorted_ids = sorted(set(line_ids.values()))
    id_mapping = {old_id: new_id for new_id, old_id in enumerate(unique_sorted_ids, start=1)}
    new_column_names = {}
    for col in df.columns:
        match = re.match(r'(\d+)(_.+)', col)
        if match:
            line_id, rest_of_name = match.groups()
            new_column_names[col] = f'{id_mapping[int(line_id)]}{rest_of_name}'
    df = df.rename(columns=new_column_names)

    # Change all column names to lowercase
    df.columns = df.columns.str.lower()

    return df

# Example usage:
# your_original_df = pd.DataFrame({...})
# final_df = process_dataframe(your_original_df)
# print(final_df)


# In[ ]:


def remove_smallest_difference_tuple(filtered_segments_30m_no_duplicates):
    # Check the number of tuples in the list
    if len(filtered_segments_30m_no_duplicates) > 2:
        # Find the tuple with the smallest difference between its elements
        # Calculate the difference for each tuple and store it along with the tuple
        differences = [(abs(a - b), (a, b)) for a, b in filtered_segments_30m_no_duplicates]
        
        # Sort the list by the difference (the first element in the tuple)
        # and select the tuple with the smallest difference
        tuple_to_remove = sorted(differences, key=lambda x: x[0])[0][1]
        
        # Remove the tuple with the smallest difference from the original list
        filtered_segments_30m_no_duplicates.remove(tuple_to_remove)

    # Return the modified list
    return filtered_segments_30m_no_duplicates

# Example usage
#filtered_segments_30m_no_duplicates = [(5, 10), (2, 8), (10, 20)]
#modified_list = remove_smallest_difference_tuple(filtered_segments_30m_no_duplicates)
#print(modified_list)



#
#30m interval start
#
#
#
#
#

def fetch_30m_interval(start_date_time_30m, end_date_time_30m):
    start_time_30m = binance.parse8601(start_date_time_30m)
    end_time_30m = binance.parse8601(end_date_time_30m)

    # Fetch data
    df_30m = fetch_data(binance, 'BTC/USDT', '30m', start_time_30m, end_time_30m, 1800000)

    #df_30m

    # Count the 30-minute intervals and add to the DataFrame
    df_with_30m_intervals = count_intervals(df_30m, 30)

    # Display the resulting DataFrame
    #df_with_30m_intervals
    return df_with_30m_intervals

# Function to remove duplicates while preserving order
def remove_duplicates_preserve_order(tuple_list):
    """
    Remove duplicate tuples from a list while preserving the order.

    Parameters:
    - tuple_list: List of tuples, where duplicates may exist.

    Returns:
    - A list of tuples with duplicates removed, order preserved.
    """
    seen = set()  # Set to store seen tuples
    result = []  # List to store the final result without duplicates
    for item in tuple_list:
        # Convert the tuple to a hashable type (itself, since tuples are hashable)
        # and check if it's not in seen set
        if item not in seen:
            # Add item to result and mark as seen
            result.append(item)
            seen.add(item)
    return result


#
#
#
#
#
#
#5 min interval start
#
#
#
#
#


def fetch_5m_interval(start_date_time_5m, end_date_time_5m):
    start_time_5m = binance.parse8601(start_date_time_5m)
    end_time_5m = binance.parse8601(end_date_time_5m)
    five_minutes_milliseconds = 300000
    # Fetch data
    df_5m = fetch_data(binance, 'BTC/USDT', '5m', start_time_5m, end_time_5m, five_minutes_milliseconds)
    #df_5m
    # For counting 5-minute intervals
    df_with_5m_intervals = count_intervals(df_5m, 5)
    # Display the resulting DataFrame
    #print(df_with_5m_intervals)
    return df_with_5m_intervals


#
#
#
#
#1 minute interval
#
#
#
#
#

def fetch_1m_interval(start_date_time_1m, end_date_time_1m):

    start_time_1m = binance.parse8601(start_date_time_1m)
    end_time_1m = binance.parse8601(end_date_time_1m)

    one_minute_milliseconds = 60000
    # Fetch data
    df_1m = fetch_data(binance, 'BTC/USDT', '1m', start_time_1m, end_time_1m, one_minute_milliseconds)

    #df_1m

    # For counting 1-minute intervals
    df_with_1m_intervals = count_intervals(df_1m, 1)
    #df_with_1m_intervals
    return df_with_1m_intervals

def add_timeframe_column_final(stacked_lastrow_df_QR):
    # Define the values for the new column in the order you specified.
    # These values correspond to each row in the DataFrame.
    timeframes = ["four_hours", "thirty_minutes", "five_minutes", "one_minute"]

    # Check if the length of your DataFrame matches the length of the 'timeframes' list.
    # This step ensures that each row will get a corresponding timeframe value.
    if len(stacked_lastrow_df_QR) == len(timeframes):
        # Assign the list of timeframes to a new column named 'timeframe'.
        stacked_lastrow_df_QR['timeframe'] = timeframes
    else:
        # If the lengths don't match, you might want to handle this case,
        # perhaps by extending the 'timeframes' list or alerting the user.
        print("The DataFrame length and timeframes list length do not match.")

    # Now, the DataFrame has the new 'timeframe' column with the specified values for each row.
    #print(stacked_lastrow_df_QR)
    return stacked_lastrow_df_QR


"""
Key Functions and Features:
1. get_current_time_now_rounded()
Rounds the current UTC time to the nearest minute.
Helps ensure consistent timestamp alignment.
2. update_dataframe_current_time(df_old, interval, interval_minutes)
Updates an existing DataFrame (df_old) with new trading data from Binance.

Converts timestamps, calculates intervals, and ensures no duplicate data.

Sub-functions:
convert_time_format_tobinance(): Converts datetime to Binance-compatible format.
update_count_intervals(): Updates a "time_count" column based on minute intervals.
update_old_data_frame(): Fetches new data, merges it with the old DataFrame, and checks for duplicates.
check_skips_in_df(): Validates the continuity of time counts and timestamps in the updated DataFrame.
3. API Interaction Functions with Freqtrade
Testing Connectivity: test_ping() sends a ping request to confirm the Freqtrade API is operational.
Managing Trades:
enter_trade(): Initiates a trade for a specified pair.
enter_trade_long() and enter_trade_short(): Specify trade direction (long/short).
get_status(): Retrieves the current status and lists trade IDs.
exit_trade(trade_id): Exits a trade based on its ID.
Improvements and Considerations:
Error Handling:

Comprehensive error handling is already present for API functions. This approach ensures robustness in production environments.
Data Integrity:

Functions like check_skips_in_df() prevent continuity issues, critical for time-series trading data.
Modular Design:

The separation of concerns between data handling, API interaction, and utility functions makes the code extensible and maintainable.
Authentication and Security:

The usage of basic authentication is straightforward, but sensitive credentials (username and password) should be managed securely, such as through environment variables or a secrets manager.
Logging and Debugging:

Add a structured logging mechanism for better traceability, especially for the Binance and Freqtrade interactions.
Unit Tests:

Write unit tests for each function to ensure correctness and catch edge cases.

"""


#
#
#
#
#
#
#
#updating data frame 
#
#
#
#
#
#
#


#tiempo ahorita redondeado
def get_current_time_now_rounded():    
    # Get the current UTC time
    now_utc = datetime.utcnow()
    
    # Round down to the nearest minute by subtracting the current seconds (and microseconds)
    rounded_utc = now_utc - timedelta(seconds=now_utc.second, microseconds=now_utc.microsecond)
    
    #print(f"Current UTC time, rounded down to the nearest minute: {rounded_utc}")
    return rounded_utc

def update_dataframe_current_time(df_old, interval, interval_minutes):
    #update dataframe timeframe 
    #cambio para usarlo en todos los timeframes intervals 
    def convert_time_format_tobinance(original_datetime):
        # Check if the input is a pandas Timestamp, convert to datetime object if true
        if isinstance(original_datetime, pd.Timestamp):
            datetime_obj = original_datetime.to_pydatetime()
        elif isinstance(original_datetime, str):
            # Specify the original format of the datetime string
            original_format = "%Y-%m-%d %H:%M:%S"
            # Parse the original datetime string into a datetime object
            datetime_obj = datetime.strptime(original_datetime, original_format)
        else:
            # If the input is already a datetime object, use it directly
            datetime_obj = original_datetime
        
        # Specify the desired output format
        # 'T' is included as a literal character in the format string
        # 'Z' indicates UTC timezone
        desired_format = "%Y-%m-%dT%H:%M:%SZ"
        
        # Convert the datetime object back into a string with the desired format
        converted_datetime_str = datetime_obj.strftime(desired_format)
        #print("convert_time_format_tobinance")
        #print(converted_datetime_str)
        
        return converted_datetime_str
    
    def update_count_intervals(df_new, df_old, interval_minutes):
        """
        Adds a column to a DataFrame counting specified minute intervals, continuing
        from the time_count of another DataFrame.
        """
        if df_new.index.min() is pd.NaT or df_old.index.max() is pd.NaT:
            print("Warning: Date-time indices contain NaT values.")
            return None
        
        # Ensure the DataFrame's index is in datetime format and sort the indexes
        for df_to_process in [df_new, df_old]:
            if not pd.api.types.is_datetime64_any_dtype(df_to_process.index):
                df_to_process.index = pd.to_datetime(df_to_process.index)
            df_to_process.sort_index(inplace=True)
        
        # Calculate the starting point for time_count in the new df_new
        if not df_old.empty:
            last_time_count = df_old['time_count'].iloc[-1]
            
            # Example check for NaN values and print for debugging
            if pd.isna(last_time_count):
                print("Warning: 'last_time_count' is NaN.")
            try:
                time_diff = (df_new.index.min() - df_old.index.max()).total_seconds() / (interval_minutes * 60)
                start_point = last_time_count + ceil(time_diff)  # Adjusted to use ceil for rounding up
            except Exception as e:
                print(f"Error calculating start_point: ")
                return None
        else:
            start_point = 0
        
        # Calculate the number of specified minute intervals since the start of the new DataFrame
        df_new['time_count'] = (((df_new.index - df_new.index.min()).total_seconds()) / (interval_minutes * 60)).astype(int) + start_point
        
        #print("update_count_intervals")
        #print(df_new)
        
        return df_new
        
    def update_old_data_frame(df_old, interval, interval_minutes):
        # Accessing the last DatetimeIndex value of the last row
        last_datetime_index_df_old_minus1 = df_old.index[-1]
        #print(last_datetime_index_df_old )
        #add 1 minute to the last to avoid fetching the same minute
        #   
        # Ensure the datetime object is properly formatted as string
        if isinstance(last_datetime_index_df_old_minus1, pd.Timestamp):
            last_datetime_index_df_old_minus1_str = last_datetime_index_df_old_minus1.strftime('%Y-%m-%d %H:%M:%S')
        else:
            last_datetime_index_df_old_minus1_str = last_datetime_index_df_old_minus1  # Assuming it's already a string
    
        # Parse the string to datetime object
        datetime_obj = datetime.strptime(last_datetime_index_df_old_minus1_str, '%Y-%m-%d %H:%M:%S')   
        datetime_obj += timedelta(minutes=interval_minutes)
        last_datetime_index_df_old = datetime_obj.strftime('%Y-%m-%d %H:%M:%S')
        #last_datetime_index_df_old = agarra el ultimo tiempo del data frame viejito le agrega 1 min o los minutos 
        #y te lo regresa en string con formato normal pero ya sumado el siguiente interval para que no repita el fetch
        
        #print("New datetime:", last_datetime_index_df_old )
        #
        # Round down to the nearest minute by subtracting the current seconds (and microseconds)
        timenow_rounded_utc = get_current_time_now_rounded()
        #timenow_rounded_utc = now_utc - timedelta(seconds=now_utc.second, microseconds=now_utc.microsecond)
        #print(f"Current UTC time, rounded down to the nearest minute: {timenow_rounded_utc}")
        
        update_start_date_time_iso = convert_time_format_tobinance(last_datetime_index_df_old)
        update_end_date_time_iso = convert_time_format_tobinance(timenow_rounded_utc)

            # Check if the start and end date-times are equal
        if update_start_date_time_iso == update_end_date_time_iso:
        # If they are equal, exit the function. Return None or a specific value indicating the early exit.
            print("Function update_old_data_frame exited early due to equal date-time values. Skipping check_skips_in_df.")
            return None
    
        # If they are not equal, continue with the function's logic
        
        update_start_time_binance = binance.parse8601(update_start_date_time_iso)
        update_end_time_binance = binance.parse8601(update_end_date_time_iso)

        interval_milliseconds = interval_minutes * 60 * 1000
        # Fetch data
        update_df_new = fetch_data(binance, 'BTC/USDT', interval, update_start_time_binance, update_end_time_binance, interval_milliseconds)
        # For counting 1-minute intervals
        update_df_new_count = update_count_intervals(update_df_new, df_old, interval_minutes)
        
        # Append new_data to df and drop duplicates based on index (DateTime)
        updated_df_new = pd.concat([df_old, update_df_new_count]).sort_index().drop_duplicates(keep='last')
        # Check for duplicates in the 'time_count' column
        if updated_df_new['time_count'].duplicated().any():
            print("Error: There are duplicate values in the 'time_count' column.")
        # Convert the index to a Series to use the .duplicated() method, checking for duplicate index values
        if pd.Series(updated_df_new.index).duplicated().any():
            print("Error: There are duplicate values in the DataFrame index.")

        #print("update_old_data_frame")
        #print(updated_df_new)
        
        return updated_df_new
    
    def check_skips_in_df(df):
        # Ensure the index is sorted
        df.sort_index(inplace=True)
        
        # Check for skipped numbers in 'time_count'
        time_count_diff = df['time_count'].diff() - 1  # We expect each diff to be 1, so subtracting 1 should give 0
        # Ignore the first row since it has no previous row to compare with
        skipped_time_counts = time_count_diff[time_count_diff != 0].iloc[1:]
        
        if skipped_time_counts.empty:
            pass
            #print("No skipped numbers in 'time_count'.")
        else:
            print("Skipped numbers found in 'time_count' at rows:", skipped_time_counts.index.tolist())
        
        # Check for skipped datetime in index (assuming the index is already in datetime format)
        datetime_diff = df.index.to_series().diff().dt.total_seconds() / 60 - 1  # Convert diff to minutes and subtract 1
        # Ignore the first row for the same reason
        skipped_datetimes = datetime_diff[datetime_diff != 0].iloc[1:]
        
        if skipped_datetimes.empty:
            pass
            #print("No skipped datetime in index.")
        else:
            print("Skipped datetime found in index at positions:")#, skipped_datetimes.index.tolist())
    
    updated_df = update_old_data_frame(df_old, interval, interval_minutes)
    
    # Check if the function returned a value indicating it did not exit early
    if updated_df is not None:
        # If result is not None, it means the function did not exit early, so we can safely call the next function
        check_skips_in_df(updated_df)
    else:
        # Handle the case where the function exited early (e.g., by skipping the call or taking some other action)
        print("Function exited early due to equal date-time values. Skipping check_skips_in_df.")

    return updated_df

def test_ping():
    """
    This function tests connectivity with the Freqtrade API by sending a ping request.
    It's crucial for verifying the API service is up before attempting further actions.
    """
    url = "http://freqtrade:8080/api/v1/ping"

    try:
        # Perform a GET request to the specified URL
        response = requests.get(url)
        # Raises an HTTPError, if one occurred
        response.raise_for_status()
        print("Success:", response.json())
    except requests.exceptions.HTTPError as errh:
        print("HTTP Error:", errh)
    except requests.exceptions.ConnectionError as errc:
        print("Error Connecting:", errc)
    except requests.exceptions.Timeout as errt:
        print("Timeout Error:", errt)
    except requests.exceptions.RequestException as err:
        print("Oops: Something Else", err)

def enter_trade():
    """
    This function sends a request to forcibly enter a trade for a specified trading pair.
    Using basic authentication for securing access to the API endpoint.
    """
    url = "http://freqtrade:8080/api/v1/forceenter"
    headers = {"Content-Type": "application/json"}
    data = {"pair": "BTC/USDT:USDT"}
    username = "private"
    password = "private"

    # POST request with Basic Authentication
    response = requests.post(url, auth=HTTPBasicAuth(username, password), headers=headers, json=data)

    if response.status_code == 200:
        print("Enter Trade Request successful.")
    else:
        print(f"Enter Trade Request failed. Status code: {response.status_code}")

def enter_trade_long():
    """
    This function sends a request to forcibly enter a trade for a specified trading pair.
    Using basic authentication for securing access to the API endpoint.
    """
    url = "http://freqtrade:8080/api/v1/forceenter"
    headers = {"Content-Type": "application/json"}
    data = {"pair": "BTC/USDT:USDT", "side": "long"}
    username = "private"
    password = "private"

    # POST request with Basic Authentication
    response = requests.post(url, auth=HTTPBasicAuth(username, password), headers=headers, json=data)

    if response.status_code == 200:
        print("Enter Trade Request successful.")
    else:
        print(f"Enter Trade Request failed. Status code: {response.status_code}")

def enter_trade_short():
    """
    This function sends a request to forcibly enter a trade for a specified trading pair.
    Using basic authentication for securing access to the API endpoint.
    """
    url = "http://freqtrade:8080/api/v1/forceenter"
    headers = {"Content-Type": "application/json"}
    data = {"pair": "BTC/USDT:USDT", "side": "short"}
    username = "private"
    password = "private"

    # POST request with Basic Authentication
    response = requests.post(url, auth=HTTPBasicAuth(username, password), headers=headers, json=data)

    if response.status_code == 200:
        print("Enter Trade Request successful.")
    else:
        print(f"Enter Trade Request failed. Status code: {response.status_code}")


def get_status():
    """
    Fetches the current trading status and returns a list of trade IDs.
    """
    url = 'http://freqtrade:8080/api/v1/status'
    username = 'private'
    password = 'private'
    
    # Make the GET request with basic authentication.
    response = requests.get(url, auth=(username, password))
    
    trade_ids = []  # Initialize an empty list to store trade IDs.
    
    # Check if the request was successful.
    if response.status_code == 200:
        data = response.json()
        
        # Collect trade_ids from the response data.
        for item in data:
            trade_id = item.get("trade_id")
            if trade_id is not None:
                trade_ids.append(trade_id)
                print("Trade ID:", trade_id)
    else:
        print(f"Error: {response.status_code} - {response.reason}")
    
    return trade_ids

def exit_trade(trade_id):
    """
    Exits a trade given a trade ID.
    """
    url = 'http://freqtrade:8080/api/v1/forceexit'
    username = 'private'
    password = 'private'

    payload = {"tradeid": str(trade_id)}  # Ensure trade_id is a string.
    headers = {'Content-Type': 'application/json'}

    try:
        # Make the POST request with basic authentication, JSON payload, and headers.
        response = requests.post(url, auth=(username, password), json=payload, headers=headers)

        # Check if the request was successful
        response.raise_for_status()

        # Print the response text
        print(response.text)

    except requests.exceptions.HTTPError as http_err:
        print(f"HTTP error occurred: {http_err}")  # HTTP error
    except Exception as err:
        print(f"Other error occurred: {err}")  # Other errors


